{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jced226/CS498Project/blob/main/CS460_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uk8W14NR1vT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Final project' #Create a new folder and add the scoliosis data google sheet and this notebook to that folder"
      ],
      "metadata": {
        "id": "qFeYCzKH12W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install neccessary packages\n",
        "\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-addons\n",
        "!pip install gspread"
      ],
      "metadata": {
        "id": "iqdc6deP59aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The Ensemble Classifier***"
      ],
      "metadata": {
        "id": "KiuJnbFqrhZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, ReLU, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.random import set_seed\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
        "random.seed(seed_value)\n",
        "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "set_seed(seed_value)\n",
        "\n",
        "\n",
        "class EnsembleNNClassifier():\n",
        "    \"\"\"EnsembleNNClassifier represents an Ensemble Model of Neural Networks\n",
        "    for binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 layers_units,\n",
        "                 n_members=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_shape (Tuple): Shape of the input data. (27,)\n",
        "            layers_units (Tuple): Tuple representing the number of units in each layer. (64, 64,)\n",
        "            n_members (int, optional): Number of members in the ensemble model. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.layers_units = layers_units\n",
        "        self.num_hidden_layers = len(layers_units)\n",
        "        self.n_members = n_members\n",
        "        self.members = []\n",
        "        self.score = 0\n",
        "        self._fitted = False\n",
        "        if len(self.members)> 0:\n",
        "            self._fitted = True\n",
        "        self._build_ensemble()\n",
        "\n",
        "  #KN added cosine_annealing\n",
        "    def cosine_annealing(self, epoch, max_lr, min_lr, T_max):\n",
        "        lr = min_lr + (max_lr - min_lr) * (1 + np.cos(np.pi * epoch / T_max)) / 2\n",
        "        return lr\n",
        "\n",
        "  #KN added compile_model\n",
        "    def _compile_model(self, model, learning_rate=0.001, min_lr=0.0001, T_max=100):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        accuracy = BinaryAccuracy()\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[accuracy])\n",
        "        # Define the Learning Rate Scheduler\n",
        "        lr_scheduler = LearningRateScheduler(lambda epoch: cosine_annealing(epoch, learning_rate, min_lr, T_max))\n",
        "\n",
        "\n",
        "    def _build_ensemble(self):\n",
        "        \"\"\"Builds the ensemble model.\n",
        "        \"\"\"\n",
        "        for model_index in range(self.n_members):\n",
        "            model_number = model_index + 1\n",
        "            self._build_model(model_number)\n",
        "\n",
        "    def compile_ensemble(self):\n",
        "        \"\"\"Compiles all models in the ensemble.\n",
        "        \"\"\"\n",
        "        for member in self.members:\n",
        "            self._compile_model(member)\n",
        "\n",
        "    def fit_evaluate(self,x_train,y_train, x_test, y_test, epochs=100, batch_size=8, verbose=0):\n",
        "        \"\"\"Trains all the models in the ensemble for a fixed number of epochs on the training data,\n",
        "        and evaluates its performance on the test data\n",
        "\n",
        "        Args:\n",
        "            x_train: Input data for training.\n",
        "            y_train: Target data for training.\n",
        "            x_test: Input data for testing.\n",
        "            y_test: Target data for testing.\n",
        "            epochs (int, optional): number of training epochs. Defaults to 100.\n",
        "            batch_size (int, optional): batch size. Defaults to 8.\n",
        "            verbose (int, optional): Verbosity mode. Defaults to 0.\n",
        "        \"\"\"\n",
        "        if self._fitted:\n",
        "            raise Exception('The model is already fitted')\n",
        "        #fit all models\n",
        "        self.members = []\n",
        "        self._build_ensemble()\n",
        "        self.compile_ensemble()\n",
        "\n",
        "        self.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "        #evaluate ensemble\n",
        "        score = self.evaluate(x_test, y_test)\n",
        "        if verbose>0:\n",
        "            print('> %.3f' % score)\n",
        "        self.score = score\n",
        "\n",
        "    def fit(self, x_train, y_train, epochs=100, batch_size=8):\n",
        "        \"\"\"Trains all the models in the ensemble for a fixed number of epochs (iterations on a dataset).\n",
        "\n",
        "        Args:\n",
        "            x_train (Input data): It could be:\n",
        "                - A Numpy array (or array-like), or a list of arrays\n",
        "                    (in case the model has multiple inputs).\n",
        "                - A TensorFlow tensor, or a list of tensors\n",
        "                    (in case the model has multiple inputs).\n",
        "                - A dict mapping input names to the corresponding array/tensors,\n",
        "                    if the model has named inputs.\n",
        "                - A `tf.data` dataset. Should return a tuple\n",
        "                    of either `(inputs, targets)` or\n",
        "                    `(inputs, targets, sample_weights)`.\n",
        "                - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
        "                    or `(inputs, targets, sample_weights)`.\n",
        "                - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
        "                    callable that takes a single argument of type\n",
        "                    `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
        "                    `DatasetCreator` should be used when users prefer to specify the\n",
        "                    per-replica batching and sharding logic for the `Dataset`.\n",
        "                    See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
        "                    information.\n",
        "            y_train (Target data.): Like the input data `x_train`,\n",
        "                it could be either Numpy array(s) or TensorFlow tensor(s).\n",
        "                It should be consistent with `x` (you cannot have Numpy inputs and\n",
        "                tensor targets, or inversely). If `x` is a dataset, generator,\n",
        "                or `keras.utils.Sequence` instance, `y` should\n",
        "                not be specified (since targets will be obtained from `x`).\n",
        "            epochs (int, optional): An epoch is an iteration over the entire `x` and `y`\n",
        "                data provided. The model is not trained for a number of iterations\n",
        "                given by `epochs`, but merely until the epoch of index `epochs`\n",
        "                is reached. Defaults to 100.\n",
        "            batch_size (int, optional): Number of samples per gradient update. Defaults to 8.\n",
        "        \"\"\"\n",
        "        if self._fitted:\n",
        "            raise Exception('The model is already fitted')\n",
        "\n",
        "        self.members = []\n",
        "        self._build_ensemble()\n",
        "        self.compile_ensemble()\n",
        "        for model in self.members:\n",
        "            self._fit_model(model,  x_train, y_train,\n",
        "                            epochs=epochs,\n",
        "                            batch_size=batch_size)\n",
        "        self._fitted = True\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generates output predictions for the input samples.\n",
        "\n",
        "        Args:\n",
        "            X: Input samples.\n",
        "\n",
        "        Returns:\n",
        "            Numpy array(s) of predictions.\n",
        "        \"\"\"\n",
        "        return np.round(self.predict_members(X))\n",
        "\n",
        "    #evaluate ensemble model\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        \"\"\"Evaluates the ensemble model's predictions\n",
        "        using balanced_accuracy_score.\n",
        "\n",
        "        Args:\n",
        "            x_test: Input data for testing.\n",
        "            y_test: Target data for testing.\n",
        "\n",
        "        Returns:\n",
        "            float: balanced_accuracy_score\n",
        "        \"\"\"\n",
        "        #make prediction\n",
        "        y_pred = self.predict(x_test)\n",
        "        #calculate accuracy\n",
        "        return balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "    def predict_members(self, X):\n",
        "        \"\"\"Generates the class probabilities of the input samples X.\n",
        "\n",
        "        Args:\n",
        "            X: Input samples.\n",
        "\n",
        "        Returns:\n",
        "            Numpy array(s) of the class probabilities of the input samples.\n",
        "        \"\"\"\n",
        "        if self._fitted:\n",
        "            y_hats = [model.predict(X) for model in self.members]\n",
        "            y_hats = np.array(y_hats)\n",
        "            # mean of predictions\n",
        "            predictions = np.median(y_hats, axis=0)\n",
        "            return predictions\n",
        "        raise Exception(\"The model should be fitted to make predictions\")\n",
        "\n",
        "\n",
        "    def _build_model(self, model_number=1):\n",
        "        \"\"\"Builds a single feed forward neural nework.\n",
        "\n",
        "        Args:\n",
        "            model_number (int, optional): The number of the model. Defaults to 1.\n",
        "        \"\"\"\n",
        "        # add the input layer\n",
        "        inputs = self._add_model_input(model_number)\n",
        "        # add hiddden layers\n",
        "        hidden_layers = self._add_hidden_layers(inputs)\n",
        "        # add the output layer\n",
        "        outputs = self._add_model_outputs(hidden_layers)\n",
        "        # add the model to the ensemble\n",
        "        self.members.append(Model(inputs=inputs, outputs=outputs, name=f\"member_{model_number}\"))\n",
        "\n",
        "    def _add_model_input(self, model_number):\n",
        "        \"\"\"Instantiates a Keras tensor and sets the input shape of the encoder.\n",
        "\n",
        "        Args:\n",
        "            model_number (int): The number of the model.\n",
        "\n",
        "        Returns:\n",
        "             A `tensor`.\n",
        "        \"\"\"\n",
        "        return Input(shape=self.input_shape, name=f\"model_{model_number}_input\")\n",
        "\n",
        "    def _add_hidden_layers(self, inputs):\n",
        "        \"\"\"Creates all the neural blocks in the neural network.\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): The input layer.\n",
        "\n",
        "        Returns:\n",
        "            _type_: the graph of layers in the neural netwok.\n",
        "        \"\"\"\n",
        "        x = inputs\n",
        "        for layer_index in range(self.num_hidden_layers):\n",
        "            x = self._add_hidden_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_hidden_layer(self, layer_index, x):\n",
        "        \"\"\"Adds a neural block to the graph of layers, consisting of a dense\n",
        "        layer + ReLU + batch normalization.\n",
        "\n",
        "        Args:\n",
        "            layer_index (int): index of the layer to create.\n",
        "            x (_type_): the graph of layers already in the neural netwok.\n",
        "\n",
        "        Returns:\n",
        "            _type_: the graph of layers in the neural netwok\n",
        "            including the newly added layer.\n",
        "        \"\"\"\n",
        "        layer_number = layer_index + 1\n",
        "        hidden_layer = Dense(self.layers_units[layer_index], name=f\"dense_layer_{layer_number}\")\n",
        "        x = hidden_layer(x)\n",
        "        x = ReLU(name=f\"relu_{layer_number}\")(x)\n",
        "        x = BatchNormalization(name=f\"batch_normalization_{layer_number}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_model_outputs(self, x):\n",
        "        \"\"\"Adds an output layer to the graph of layers in the network.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): the graph of layers in the network.\n",
        "\n",
        "        Returns:\n",
        "            tensor: The graph of layers in the network plus the output layer.\n",
        "        \"\"\"\n",
        "        logits = Dense(units=1, name=f\"model_logits\")(x)\n",
        "        output = Activation('sigmoid', name=f\"sigmoid_layer\")(logits)\n",
        "        return output\n",
        "\n",
        "#KN changed fit_model\n",
        "    def _fit_model(self, model, x_train, y_train, epochs=100, batch_size=8):\n",
        "        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n",
        "\n",
        "        Args:\n",
        "            model: Model to train.\n",
        "            x_train (Input data): It could be:\n",
        "                - A Numpy array (or array-like), or a list of arrays\n",
        "                    (in case the model has multiple inputs).\n",
        "                - A TensorFlow tensor, or a list of tensors\n",
        "                    (in case the model has multiple inputs).\n",
        "                - A dict mapping input names to the corresponding array/tensors,\n",
        "                    if the model has named inputs.\n",
        "                - A `tf.data` dataset. Should return a tuple\n",
        "                    of either `(inputs, targets)` or\n",
        "                    `(inputs, targets, sample weights)`.\n",
        "                - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
        "                    or `(inputs, targets, sample weights)`.\n",
        "                - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
        "                    callable that takes a single argument of type\n",
        "                    `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
        "                    `DatasetCreator` should be used when users prefer to specify the\n",
        "                    per-replica batching and sharding logic for the `Dataset`.\n",
        "                    See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
        "                    information.\n",
        "            y_train (Target data.): Like the input data `x_train`,\n",
        "                it could be either Numpy array(s) or TensorFlow tensor(s).\n",
        "                It should be consistent with `x` (you cannot have Numpy inputs and\n",
        "                tensor targets, or inversely). If `x` is a dataset, generator,\n",
        "                or `keras.utils.Sequence` instance, `y` should\n",
        "                not be specified (since targets will be obtained from `x`).\n",
        "            epochs (int, optional): An epoch is an iteration over the entire `x` and `y`\n",
        "                data provided.\n",
        "                The model is not trained for a number of iterations\n",
        "                given by `epochs`, but merely until the epoch\n",
        "                of index `epochs` is reached. Defaults to 100.\n",
        "            batch_size (int, optional): Number of samples per gradient update. Defaults to 8.\n",
        "        \"\"\"\n",
        "        # Define the Learning Rate Scheduler and Early Stopping\n",
        "        lr_scheduler = LearningRateScheduler(lambda epoch: self.cosine_annealing(epoch, 0.001, 0.0001, 100))\n",
        "        monitor = EarlyStopping(monitor='binary_accuracy', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "        callbacks = [monitor, lr_scheduler]\n",
        "\n",
        "        model.fit(x_train, y_train,\n",
        "                  epochs=epochs,\n",
        "                  batch_size=batch_size,\n",
        "                  shuffle=True,\n",
        "                  callbacks=callbacks,\n",
        "                  verbose=0)\n",
        "        return model\n",
        "\n",
        "# Below is the original\n",
        "#   def _fit_model(self,model, x_train, y_train, epochs=100, batch_size=8):\n",
        "        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n",
        "\n",
        "        Args:\n",
        "            model: Model to train.\n",
        "            x_train (Input data): It could be:\n",
        "                - A Numpy array (or array-like), or a list of arrays\n",
        "                    (in case the model has multiple inputs).\n",
        "                - A TensorFlow tensor, or a list of tensors\n",
        "                    (in case the model has multiple inputs).\n",
        "                - A dict mapping input names to the corresponding array/tensors,\n",
        "                    if the model has named inputs.\n",
        "                - A `tf.data` dataset. Should return a tuple\n",
        "                    of either `(inputs, targets)` or\n",
        "                    `(inputs, targets, sample_weights)`.\n",
        "                - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
        "                    or `(inputs, targets, sample_weights)`.\n",
        "                - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
        "                    callable that takes a single argument of type\n",
        "                    `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
        "                    `DatasetCreator` should be used when users prefer to specify the\n",
        "                    per-replica batching and sharding logic for the `Dataset`.\n",
        "                    See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
        "                    information.\n",
        "            y_train (Target data.): Like the input data `x_train`,\n",
        "                it could be either Numpy array(s) or TensorFlow tensor(s).\n",
        "                It should be consistent with `x` (you cannot have Numpy inputs and\n",
        "                tensor targets, or inversely). If `x` is a dataset, generator,\n",
        "                or `keras.utils.Sequence` instance, `y` should\n",
        "                not be specified (since targets will be obtained from `x`).\n",
        "            epochs (int, optional): An epoch is an iteration over the entire `x` and `y`\n",
        "                data provided.\n",
        "                The model is not trained for a number of iterations\n",
        "                given by `epochs`, but merely until the epoch\n",
        "                of index `epochs` is reached. Defaults to 100.\n",
        "            batch_size (int, optional): Number of samples per gradient update. Defaults to 8.\n",
        "\n",
        "        Returns:\n",
        "            A `History` object. Its `History.history` attribute is\n",
        "            a record of training loss values and metrics values\n",
        "            at successive epochs.\n",
        "        \"\"\"\n",
        "#        monitor = EarlyStopping(monitor='binary_accuracy', min_delta=1e-3,\n",
        "#                        patience=5, verbose=1, mode='auto')\n",
        "        #fit model\n",
        "#        model.fit(x_train, y_train,\n",
        "#                  epochs=epochs,\n",
        "#                  batch_size=batch_size,\n",
        "#                  shuffle=True,\n",
        "#                  callbacks=[monitor],\n",
        "#                  verbose=0)\n",
        "#        return model\n",
        "\n",
        "\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder_path):\n",
        "        \"\"\"Creates a folder if it does not exist in the given path.\n",
        "\n",
        "        Args:\n",
        "            folder_path (str): Path of the folder.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "    def _save_parameters(self, folder_path):\n",
        "        \"\"\"Saves the parameters of the model.\n",
        "\n",
        "        Args:\n",
        "            folder_path (str): Path of the folder where the parameters will be saved.\n",
        "        \"\"\"\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.layers_units,\n",
        "            self.n_members\n",
        "        ]\n",
        "        save_path = os.path.join(folder_path, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, folder_path):\n",
        "        \"\"\"Saves the weights of the model.\n",
        "\n",
        "        Args:\n",
        "            folder_path (str):  Path of the folder where the weights will be saved.\n",
        "        \"\"\"\n",
        "        for idx, model in zip(range(self.n_members), self.members):\n",
        "            file_path = os.path.join(folder_path, f\"weights_{idx}.h5\")\n",
        "            model.save_weights(file_path)\n",
        "\n",
        "    def save(self, folder_path=\".\"):\n",
        "        \"\"\"Creates a folder if it does not exist in the given path.\n",
        "        And, saves the parameters and weights of the model in `folder_path`.\n",
        "\n",
        "        Args:\n",
        "            folder_path (str, optional): Path of the folder.\n",
        "            Defaults to the path of the current directory.\n",
        "        \"\"\"\n",
        "        self._create_folder_if_it_doesnt_exist(folder_path)\n",
        "        self._save_parameters(folder_path)\n",
        "        self._save_weights(folder_path)\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        \"\"\"Loads the weights of the model saved in the given path.\n",
        "\n",
        "        Args:\n",
        "            weights_path (str): Path of the file where the weights are saved.\n",
        "        \"\"\"\n",
        "        for model in self.members:\n",
        "            model.load_weights(weights_path)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, folder_path=\".\"):\n",
        "        \"\"\"Loads the model from the given folder `folder_path`.\n",
        "\n",
        "        Args:\n",
        "            folder_path (str, optional): Path of the folder.\n",
        "            Defaults to the path of the current directory.\n",
        "\n",
        "        Returns:\n",
        "            EnsembleNNClassifier: The model saved in the given folder.\n",
        "        \"\"\"\n",
        "        parameters_path = os.path.join(folder_path, \"parameters.pkl\")\n",
        "        with open(parameters_path, \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        ensemble_nn = cls(*parameters)\n",
        "\n",
        "        for idx, model in zip(range(ensemble_nn.n_members), ensemble_nn.members):\n",
        "            weights_path = os.path.join(folder_path, f\"weights_{idx}.h5\")\n",
        "            model.load_weights(weights_path)\n",
        "        ensemble_nn._fitted = True\n",
        "        return ensemble_nn\n"
      ],
      "metadata": {
        "id": "DPcyqHIaXIa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install the gspread package so the code can read from a gsheet file**"
      ],
      "metadata": {
        "id": "hPqFpb0ZwzxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Ensemble**\n",
        "\n",
        "This trains the model and uses the EnsembleNNClassifier"
      ],
      "metadata": {
        "id": "9N4_ddJw9vNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from numpy import max, random, array, round\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default, iam\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "def load_scoliosis_data():\n",
        "    creds, project = default()\n",
        "    client = gspread.authorize(creds.with_scopes(['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']))\n",
        "    spreadsheet = client.open('scoliosis_data')\n",
        "    worksheet = spreadsheet.worksheet('health-scoliotic patients')\n",
        "    data = worksheet.get_all_values()\n",
        "    dataset = DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "\n",
        "    X = dataset.drop(['Patients','y'], axis=1)\n",
        "    y = dataset['y']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X_train, y_train, X_test, y_test = load_scoliosis_data()\n",
        "\n",
        "    # Cast X_train and X_test to floats\n",
        "    X_train = X_train.astype(float)\n",
        "    X_test = X_test.astype(float)\n",
        "\n",
        "    # Cast y_train and y_test to floats\n",
        "    y_train = y_train.astype(float)\n",
        "    y_test = y_test.astype(float)\n",
        "\n",
        "    clf = EnsembleNNClassifier(input_shape=X_train.iloc[0].shape, layers_units=(64,64,), n_members=4)\n",
        "\n",
        "    print('Start training...')\n",
        "    clf.fit(X_train, y_train,\n",
        "                     epochs= 100,\n",
        "                     batch_size= 8)\n",
        "    print('End training')\n",
        "    acc = clf.evaluate(X_test, y_test)\n",
        "    print(f'Balanced accuracy of the model: {round(acc,3)*100}')\n",
        "    print('Saving the model')\n",
        "    clf.save('model')\n",
        "    print('Loading the model')\n",
        "    loaded_clf = EnsembleNNClassifier.load('model')\n",
        "    print('Evaluating the loaded model:', loaded_clf.evaluate(X_test, y_test))\n",
        "\n",
        "    ensemble_predictions = [x[0] for x in loaded_clf.predict(X_test.iloc[:10])]\n",
        "\n",
        "    print('Making predictions using the loaded model',ensemble_predictions )\n"
      ],
      "metadata": {
        "id": "eVKY4Rds1iVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This bit of code is visualizng the predictions of the first 10 patients and showing the probability if the patient has scoliosis. 1 meaning they do have scoliosis, and 0 meaning they do not have scoliosis.**"
      ],
      "metadata": {
        "id": "IaibhE91wWBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the model\n",
        "loaded_clf = EnsembleNNClassifier.load('model')\n",
        "\n",
        "# Load X_test\n",
        "X_test, _ = load_scoliosis_data()[-2:]\n",
        "\n",
        "# Cast X_test to float\n",
        "X_test = X_test.astype(float)\n",
        "\n",
        "\n",
        "# Make predictions for the first 10 rows in the test set\n",
        "predictions = loaded_clf.predict(X_test.iloc[:10])\n",
        "\n",
        "# Extract predicted probabilities of having scoliosis\n",
        "probabilities = predictions[:, 0]\n",
        "\n",
        "# Create a bar graph to visualize the predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(10), probabilities, color='blue')\n",
        "plt.xlabel('Patients')\n",
        "plt.ylabel('Probabilities Patient has Scoliosis')\n",
        "plt.title('Predicted Probabilities for the First 10 Patients')\n",
        "plt.xticks(range(10), [f'Patient {i}' for i in range(1, 11)])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VeSgyg0ztW3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**\n",
        "\n",
        "Contains the load data"
      ],
      "metadata": {
        "id": "J9rAQsNr91ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default, iam\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "def load_scoliosis_data():\n",
        "    creds, project = default()\n",
        "    client = gspread.authorize(creds.with_scopes(['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']))\n",
        "    spreadsheet = client.open('scoliosis_data')\n",
        "    worksheet = spreadsheet.worksheet('health-scoliotic patients')\n",
        "    data = worksheet.get_all_values()\n",
        "    dataset = DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "    X = dataset.drop(['Patients','y'], axis=1)\n",
        "    y = dataset['y']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X_train, y_train, X_test, y_test = load_scoliosis_data()\n",
        "\n",
        "    # Cast X_train and X_test to floats\n",
        "    X_train = X_train.astype(float)\n",
        "    X_test = X_test.astype(float)\n",
        "\n",
        "    # Cast y_train and y_test to floats\n",
        "    y_train = y_train.astype(float)\n",
        "    y_test = y_test.astype(float)\n",
        "\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "    print(\"Shape of y_train:\", y_train.shape)\n",
        "    print(\"Shape of X_test:\", X_test.shape)\n",
        "    print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "\n",
        "    print('Loading the model')\n",
        "    # Assuming you have already loaded the EnsembleNNClassifier\n",
        "    loaded_clf = EnsembleNNClassifier.load('model')\n",
        "    status_dict = {1: 'has Scoliosis', 0: 'is Healthy'}\n",
        "\n",
        "    # feature_names = 'x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20','x21','x22','x23','x24','x25','x26','x27'\n",
        "\n",
        "    # Making predictions for the first 10 rows in the test set\n",
        "    ensemble_predictions = [f'Subject {i} ' + status_dict[int(x[0])] for i, x in enumerate(loaded_clf.predict(X_test.iloc[:10]))]\n",
        "    print()\n",
        "    print('-------------------------------------------------------------------------' )\n",
        "    print()\n",
        "    print('Making predictions for the first 10 rows in the test set' )\n",
        "    print(ensemble_predictions)\n",
        "    print()\n",
        "    print('-------------------------------------------------------------------------' )\n",
        "    print()\n",
        "    # Making predictions for two candidate subjects\n",
        "    candidate_subject = [30.525,-12.75,-3,1.66,12.325,0.395,-0.000718327,-0.3780369,-0.672629766,-0.810873309,-0.909343173,67.085,26.475,41.875,32.18,17.385,3.95,-6.335,0.72,-6.335,7.055,-3.345,2.95,6.23,6.23,-3.33,9.56]\n",
        "    candidate_subject_1 = [8.37,-12,6,2.12,12.93,3.35,0.007217427,-0.329721479,-0.562601289,-0.787001093,-0.932781819,63.14,37.33,45.21,26.18,10.71,2.17,5.28,5.28,-1.32,6.61,0.91,1.58,-3.13,1.94,-3.13,5.07]\n",
        "    prediction = loaded_clf.predict([candidate_subject])[0]\n",
        "    status = status_dict[int(prediction)]\n",
        "    print(f'The candidate subject {status}')\n",
        "    print()\n",
        "    print('-------------------------------------------------------------------------' )\n",
        "    print()\n",
        "    prediction = loaded_clf.predict([candidate_subject_1])[0]\n",
        "    status = status_dict[int(prediction)]\n",
        "    print(f'The candidate subject {status}')\n",
        "    print()\n",
        "    print('-------------------------------------------------------------------------' )\n",
        "    print()\n",
        "\n",
        "    # Randomly select a candidate subject\n",
        "    random_index = random.randint(0, len(X_test) - 1)\n",
        "    random_patient_number = X_test.index[random_index]  # Extracting the patient number\n",
        "    random_candidate = X_test.iloc[random_index]\n",
        "    random_candidate_label = y_test.iloc[random_index]\n",
        "\n",
        "    # Reshape the random candidate to match the input shape of the model\n",
        "    random_candidate = array(random_candidate).reshape(1, -1)\n",
        "\n",
        "    # Make predictions for the random candidate\n",
        "    prediction = loaded_clf.predict(random_candidate)[0]\n",
        "    status = status_dict[int(prediction)]\n",
        "    print(f'The randomly selected candidate subject {status}.')\n",
        "    print(f'Patient number: {random_patient_number}')  # Printing the patient number\n",
        "\n"
      ],
      "metadata": {
        "id": "DHPwsSxyq8Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creates .png files showing the architecture of each one of the models**"
      ],
      "metadata": {
        "id": "G1Yek0pwxB1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Assuming you have loaded the ensemble model\n",
        "loaded_clf = EnsembleNNClassifier.load('model')\n",
        "\n",
        "# Visualize each individual model in the ensemble\n",
        "for i, model in enumerate(loaded_clf.members):\n",
        "    plot_model(model, to_file=f'model_{i}_architecture.png', show_shapes=True)\n"
      ],
      "metadata": {
        "id": "nY889-QBrOSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging Method**"
      ],
      "metadata": {
        "id": "hGHIhWXArrMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define base classifier\n",
        "base_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Create bagging classifier\n",
        "bagging_clf = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the BaggingClassifier on training data\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the BaggingClassifier\n",
        "accuracy = bagging_clf.score(X_test, y_test)\n",
        "print(\"Bagging Classifier Accuracy:\", (accuracy*100).round(3))\n"
      ],
      "metadata": {
        "id": "4NYoRrlepc4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting Method**"
      ],
      "metadata": {
        "id": "x9k75MHSroMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Create AdaBoost classifier\n",
        "adaboost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Fit the AdaBoostClassifier on training data\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the AdaBoostClassifier\n",
        "accuracy = adaboost_clf.score(X_test, y_test)\n",
        "print(\"AdaBoost Classifier Accuracy:\", (accuracy*100).round(3))\n"
      ],
      "metadata": {
        "id": "I9K6pPQcpggU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stacking Method**"
      ],
      "metadata": {
        "id": "tJBrCxS6rlJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Define base classifiers\n",
        "base_classifiers = [\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "]\n",
        "\n",
        "# Train base classifiers using cross-validation\n",
        "base_predictions = []\n",
        "for clf in base_classifiers:\n",
        "    predictions = cross_val_predict(clf, X_train, y_train, cv=5, method='predict_proba')\n",
        "    base_predictions.append(predictions)\n",
        "\n",
        "# Concatenate base predictions\n",
        "X_meta_train = np.concatenate(base_predictions, axis=1)\n",
        "\n",
        "# Train meta-model\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(X_meta_train, y_train)\n",
        "\n",
        "# Generate meta features for test set\n",
        "base_test_predictions = []\n",
        "for clf in base_classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict_proba(X_test)\n",
        "    base_test_predictions.append(predictions)\n",
        "\n",
        "X_meta_test = np.concatenate(base_test_predictions, axis=1)\n",
        "\n",
        "# Make predictions using meta-model\n",
        "ensemble_predictions = meta_model.predict(X_meta_test)\n",
        "accuracy = metrics.accuracy_score(y_test, ensemble_predictions)\n",
        "print(\"Stacking Ensemble Accuracy:\", (accuracy*100).round(3))\n",
        "\n"
      ],
      "metadata": {
        "id": "6ykhMqbXpxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Classifier**"
      ],
      "metadata": {
        "id": "K1WdDl3oYDL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "class MLPModelClassifier:\n",
        "    def __init__(self, input_shape, hidden_layer_sizes=(64, 64), n_estimators=5):\n",
        "        self.input_shape = input_shape\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.n_estimators = n_estimators\n",
        "        self.classifiers = []\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        for _ in range(self.n_estimators):\n",
        "            classifier = MLPClassifier(hidden_layer_sizes=self.hidden_layer_sizes)\n",
        "            classifier.fit(X_train, y_train)\n",
        "            self.classifiers.append(classifier)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for classifier in self.classifiers:\n",
        "            predictions.append(classifier.predict(X))\n",
        "        return np.median(predictions, axis=0)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.predict(X_test)\n",
        "        return balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "    def save(self, folder_path=\".\"):\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "        parameters = {\n",
        "            \"input_shape\": self.input_shape,\n",
        "            \"hidden_layer_sizes\": self.hidden_layer_sizes,\n",
        "            \"n_estimators\": self.n_estimators\n",
        "        }\n",
        "        with open(os.path.join(folder_path, \"parameters.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(parameters, f)\n",
        "        for idx, classifier in enumerate(self.classifiers):\n",
        "            with open(os.path.join(folder_path, f\"classifier_{idx}.pkl\"), \"wb\") as f:\n",
        "                pickle.dump(classifier, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, folder_path=\".\"):\n",
        "        with open(os.path.join(folder_path, \"parameters.pkl\"), \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        ensemble = cls(parameters[\"input_shape\"], parameters[\"hidden_layer_sizes\"], parameters[\"n_estimators\"])\n",
        "        for idx in range(ensemble.n_estimators):\n",
        "            with open(os.path.join(folder_path, f\"classifier_{idx}.pkl\"), \"rb\") as f:\n",
        "                classifier = pickle.load(f)\n",
        "                ensemble.classifiers.append(classifier)\n",
        "        return ensemble\n"
      ],
      "metadata": {
        "id": "TYHO7HF-_zdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main that Runs the MLP Classifier**"
      ],
      "metadata": {
        "id": "As8fZxchYM_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default, iam\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "def load_scoliosis_data():\n",
        "    creds, project = default()\n",
        "    client = gspread.authorize(creds.with_scopes(['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']))\n",
        "    spreadsheet = client.open('scoliosis_data')\n",
        "    worksheet = spreadsheet.worksheet('health-scoliotic patients')\n",
        "    data = worksheet.get_all_values()\n",
        "    dataset = DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "    X = dataset.drop(['Patients','y'], axis=1)\n",
        "    y = dataset['y'].astype(int) #KN added .astype(int). This is to ensure y is of integer type.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X_train, y_train, X_test, y_test = load_scoliosis_data()\n",
        "\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "    print(\"Shape of y_train:\", y_train.shape)\n",
        "    print(\"Shape of X_test:\", X_test.shape)\n",
        "    print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "    # Initialize MLPClassifier\n",
        "    clf = MLPModelClassifier(input_shape=(27,), hidden_layer_sizes=(64, 64), n_estimators=5)\n",
        "\n",
        "   # clf.predict() - old code.\n",
        "\n",
        "#KN added below codes instead of clf.predict()\n",
        "    # Fit the model with training data\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict using the trained model on the test data\n",
        "    predictions = clf.predict(X_test)\n",
        "    print(\"Predictions:\", predictions)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = clf.evaluate(X_test, y_test)\n",
        "    print(\"Balanced accuracy of the model:\", accuracy*100)\n",
        "\n"
      ],
      "metadata": {
        "id": "idzzzw-y-yez"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}